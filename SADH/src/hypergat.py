#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¶…å›¾æ³¨æ„åŠ›å·ç§¯å±‚ (HyperGAT)
å®ç°å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„è¶…å›¾å·ç§¯
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class HyperGATConv(nn.Module):
    """è¶…å›¾æ³¨æ„åŠ›å·ç§¯å±‚"""
    
    def __init__(self, in_dim, out_dim, num_heads=8, dropout=0.1, alpha=0.2):
        super(HyperGATConv, self).__init__()
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.num_heads = num_heads
        self.dropout = dropout
        self.alpha = alpha
        
        # ç¡®ä¿è¾“å‡ºç»´åº¦èƒ½è¢«å¤´æ•°æ•´é™¤
        assert out_dim % num_heads == 0
        self.head_dim = out_dim // num_heads
        
        # çº¿æ€§å˜æ¢å±‚
        self.W = nn.Linear(in_dim, out_dim, bias=False)
        
        # æ³¨æ„åŠ›å‚æ•°
        self.attention = nn.Parameter(torch.Tensor(1, num_heads, 2 * self.head_dim))
        
        # è¾“å‡ºåç½®
        self.bias = nn.Parameter(torch.Tensor(out_dim))
        
        # åˆå§‹åŒ–å‚æ•°
        self._init_parameters()
    
    def _init_parameters(self):
        """åˆå§‹åŒ–å‚æ•°"""
        nn.init.xavier_uniform_(self.W.weight)
        nn.init.xavier_uniform_(self.attention)
        nn.init.zeros_(self.bias)
    
    def forward(self, X, H):
        """
        å‰å‘ä¼ æ’­
        Args:
            X: èŠ‚ç‚¹ç‰¹å¾ [N, in_dim]
            H: è¶…å›¾å…³è”çŸ©é˜µ [N, E]
        Returns:
            out: æ›´æ–°åçš„èŠ‚ç‚¹ç‰¹å¾ [N, out_dim]
        """
        N, E = H.shape
        
        # çº¿æ€§å˜æ¢
        X_transformed = self.W(X)  # [N, out_dim]
        
        # é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
        X_heads = X_transformed.view(N, self.num_heads, self.head_dim)  # [N, num_heads, head_dim]
        
        # è®¡ç®—è¶…è¾¹å†…çš„æ³¨æ„åŠ›
        attention_scores = self._compute_attention_scores(X_heads, H)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        out_heads = self._apply_attention(X_heads, H, attention_scores)
        
        # åˆå¹¶å¤šå¤´
        out = out_heads.view(N, self.out_dim)
        
        # æ·»åŠ åç½®
        out = out + self.bias
        
        return out
    
    def _compute_attention_scores(self, X_heads, H):
        """
        è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        Args:
            X_heads: å¤šå¤´èŠ‚ç‚¹ç‰¹å¾ [N, num_heads, head_dim]
            H: è¶…å›¾å…³è”çŸ©é˜µ [N, E]
        Returns:
            attention_scores: æ³¨æ„åŠ›åˆ†æ•° [E, num_heads, max_degree]
        """
        N, E = H.shape
        device = X_heads.device
        
        # è®¡ç®—æ¯ä¸ªè¶…è¾¹çš„æœ€å¤§åº¦æ•°
        degrees = H.sum(dim=0)  # [E]
        max_degree = int(degrees.max().item())
        
        # åˆå§‹åŒ–æ³¨æ„åŠ›åˆ†æ•°
        attention_scores = torch.zeros(E, self.num_heads, max_degree, device=device)
        
        for e in range(E):
            # è·å–è¶…è¾¹eä¸­çš„èŠ‚ç‚¹
            nodes_in_edge = torch.where(H[:, e] > 0)[0]  # èŠ‚ç‚¹ç´¢å¼•
            degree = len(nodes_in_edge)
            
            if degree == 0:
                continue
            
            # è·å–è¶…è¾¹å†…èŠ‚ç‚¹çš„ç‰¹å¾
            edge_features = X_heads[nodes_in_edge]  # [degree, num_heads, head_dim]
            
            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
            for i in range(degree):
                # å½“å‰èŠ‚ç‚¹ä¸è¶…è¾¹å†…æ‰€æœ‰èŠ‚ç‚¹çš„æ³¨æ„åŠ›
                current_node = edge_features[i:i+1]  # [1, num_heads, head_dim]
                all_nodes = edge_features  # [degree, num_heads, head_dim]
                
                # æ‹¼æ¥ç‰¹å¾
                concat_features = torch.cat([
                    current_node.expand(degree, -1, -1),  # [degree, num_heads, head_dim]
                    all_nodes  # [degree, num_heads, head_dim]
                ], dim=-1)  # [degree, num_heads, 2*head_dim]
                
                # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
                scores = torch.sum(
                    self.attention * concat_features, 
                    dim=-1
                )  # [degree, num_heads]
                
                # åº”ç”¨LeakyReLU
                scores = F.leaky_relu(scores, negative_slope=self.alpha)
                
                # å­˜å‚¨æ³¨æ„åŠ›åˆ†æ•°
                attention_scores[e, :, i] = scores[i]
        
        return attention_scores
    
    def _apply_attention(self, X_heads, H, attention_scores):
        """
        åº”ç”¨æ³¨æ„åŠ›æƒé‡
        Args:
            X_heads: å¤šå¤´èŠ‚ç‚¹ç‰¹å¾ [N, num_heads, head_dim]
            H: è¶…å›¾å…³è”çŸ©é˜µ [N, E]
            attention_scores: æ³¨æ„åŠ›åˆ†æ•° [E, num_heads, max_degree]
        Returns:
            out_heads: æ›´æ–°åçš„å¤šå¤´ç‰¹å¾ [N, num_heads, head_dim]
        """
        N, E = H.shape
        device = X_heads.device
        
        # åˆå§‹åŒ–è¾“å‡º
        out_heads = torch.zeros_like(X_heads)
        
        # è®¡ç®—æ¯ä¸ªè¶…è¾¹çš„æœ€å¤§åº¦æ•°
        degrees = H.sum(dim=0)  # [E]
        max_degree = int(degrees.max().item())
        
        for e in range(E):
            # è·å–è¶…è¾¹eä¸­çš„èŠ‚ç‚¹
            nodes_in_edge = torch.where(H[:, e] > 0)[0]  # èŠ‚ç‚¹ç´¢å¼•
            degree = len(nodes_in_edge)
            
            if degree == 0:
                continue
            
            # è·å–è¶…è¾¹å†…èŠ‚ç‚¹çš„ç‰¹å¾
            edge_features = X_heads[nodes_in_edge]  # [degree, num_heads, head_dim]
            
            # è·å–æ³¨æ„åŠ›æƒé‡
            edge_attention = attention_scores[e, :, :degree]  # [num_heads, degree]
            
            # åº”ç”¨softmaxå½’ä¸€åŒ–
            edge_attention = F.softmax(edge_attention, dim=-1)  # [num_heads, degree]
            
            # åº”ç”¨dropout
            edge_attention = F.dropout(edge_attention, p=self.dropout, training=self.training)
            
            # åŠ æƒèšåˆ
            # edge_attention: [num_heads, degree]
            # edge_features: [degree, num_heads, head_dim]
            # éœ€è¦è°ƒæ•´ç»´åº¦ä»¥è¿›è¡Œæ‰¹é‡çŸ©é˜µä¹˜æ³•
            edge_attention = edge_attention.unsqueeze(-1)  # [num_heads, degree, 1]
            edge_features = edge_features.transpose(0, 1)  # [num_heads, degree, head_dim]
            
            aggregated = torch.sum(edge_attention * edge_features, dim=1)  # [num_heads, head_dim]
            
            # å°†èšåˆç»“æœåˆ†é…ç»™è¶…è¾¹å†…çš„æ‰€æœ‰èŠ‚ç‚¹
            for i, node_idx in enumerate(nodes_in_edge):
                out_heads[node_idx] += aggregated
        
        return out_heads


class MultiLayerHyperGAT(nn.Module):
    """å¤šå±‚è¶…å›¾æ³¨æ„åŠ›ç½‘ç»œ"""
    
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, 
                 num_heads=8, dropout=0.1, alpha=0.2):
        super(MultiLayerHyperGAT, self).__init__()
        self.num_layers = num_layers
        
        # æ„å»ºå¤šå±‚
        self.layers = nn.ModuleList()
        
        # ç¬¬ä¸€å±‚
        self.layers.append(HyperGATConv(
            input_dim, hidden_dim, num_heads, dropout, alpha
        ))
        
        # ä¸­é—´å±‚
        for _ in range(num_layers - 2):
            self.layers.append(HyperGATConv(
                hidden_dim, hidden_dim, num_heads, dropout, alpha
            ))
        
        # æœ€åä¸€å±‚
        if num_layers > 1:
            self.layers.append(HyperGATConv(
                hidden_dim, output_dim, num_heads, dropout, alpha
            ))
        
        # æ¿€æ´»å‡½æ•°
        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, X, H):
        """
        å‰å‘ä¼ æ’­
        Args:
            X: è¾“å…¥ç‰¹å¾ [N, input_dim]
            H: è¶…å›¾å…³è”çŸ©é˜µ [N, E]
        Returns:
            out: è¾“å‡ºç‰¹å¾ [N, output_dim]
        """
        x = X
        
        for i, layer in enumerate(self.layers):
            x = layer(x, H)
            
            # é™¤äº†æœ€åä¸€å±‚ï¼Œéƒ½åº”ç”¨æ¿€æ´»å‡½æ•°å’Œdropout
            if i < len(self.layers) - 1:
                x = self.activation(x)
                x = self.dropout(x)
        
        return x


def test_hypergat():
    """æµ‹è¯•HyperGATå±‚"""
    print("ğŸ§ª æµ‹è¯•HyperGATå±‚...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    N, E, in_dim, out_dim = 10, 5, 64, 128
    X = torch.randn(N, in_dim)
    H = torch.randint(0, 2, (N, E)).float()
    
    print(f"è¾“å…¥ç‰¹å¾å½¢çŠ¶: {X.shape}")
    print(f"è¶…å›¾å…³è”çŸ©é˜µå½¢çŠ¶: {H.shape}")
    
    # æµ‹è¯•å•å±‚HyperGAT
    hypergat = HyperGATConv(in_dim, out_dim, num_heads=8)
    out = hypergat(X, H)
    print(f"å•å±‚è¾“å‡ºå½¢çŠ¶: {out.shape}")
    
    # æµ‹è¯•å¤šå±‚HyperGAT
    ml_hypergat = MultiLayerHyperGAT(in_dim, 64, out_dim, num_layers=2)
    out_ml = ml_hypergat(X, H)
    print(f"å¤šå±‚è¾“å‡ºå½¢çŠ¶: {out_ml.shape}")
    
    print("âœ… HyperGATæµ‹è¯•æˆåŠŸ!")


if __name__ == "__main__":
    test_hypergat() 
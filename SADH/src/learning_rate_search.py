#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å­¦ä¹ ç‡æœç´¢è„šæœ¬
ç³»ç»Ÿæµ‹è¯•ä¸åŒå­¦ä¹ ç‡ï¼Œæ‰¾åˆ°æœ€ä½³é…ç½®
"""

import os
import sys
import yaml
import torch
import numpy as np
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from run_simplified_experimrnt import (
    load_config, load_handwritten_data, build_simplified_hypergraph,
    initialize_features_according_to_paper, split_train_test
)
from models import SimpleHyperGCN
from jihl_module import JIHLModule

# ç›´æ¥å®šä¹‰SimpleTrainerç±»ï¼Œé¿å…å¯¼å…¥é—®é¢˜
class SimpleTrainer:
    def __init__(self, lr=0.001, weight_decay=1e-4, tau=0.07):
        self.lr = lr
        self.weight_decay = weight_decay
        self.tau = tau
    
    def contrastive_loss(self, features, labels):
        """è®¡ç®—å¯¹æ¯”æŸå¤± - InfoNCEæŸå¤±"""
        # å½’ä¸€åŒ–ç‰¹å¾
        features = torch.nn.functional.normalize(features, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.matmul(features, features.T) / self.tau
        
        # åˆ›å»ºæ ‡ç­¾çŸ©é˜µï¼šç›¸åŒæ ‡ç­¾ä¸ºæ­£æ ·æœ¬å¯¹
        labels = labels.contiguous().view(-1, 1)
        mask = torch.eq(labels, labels.T).float()
        
        # ç§»é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±ä¸è‡ªå·±çš„ç›¸ä¼¼åº¦ï¼‰
        mask = mask - torch.eye(mask.shape[0], device=mask.device)
        
        # è®¡ç®—æ­£æ ·æœ¬å¯¹å’Œè´Ÿæ ·æœ¬å¯¹
        positives = similarity_matrix * mask
        negatives = similarity_matrix * (1 - mask)
        
        # è®¡ç®—æ­£æ ·æœ¬å¯¹æ•°é‡
        num_positives = mask.sum(dim=1)
        
        # è®¡ç®—å¯¹æ¯”æŸå¤±
        logits = torch.cat([positives, negatives], dim=1)
        labels_contrastive = torch.zeros(logits.shape[0], device=logits.device, dtype=torch.long)
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¯¹æ¯”æŸå¤±
        exp_logits = torch.exp(logits)
        log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True))
        
        # åªå¯¹æ­£æ ·æœ¬å¯¹è®¡ç®—æŸå¤±
        mean_log_prob_pos = (mask * log_prob[:, :mask.shape[1]]).sum(dim=1) / (num_positives + 1e-8)
        
        # æœ€ç»ˆæŸå¤±
        contrastive_loss = -mean_log_prob_pos.mean()
        
        # è°ƒè¯•ä¿¡æ¯
        if torch.isnan(contrastive_loss) or torch.isinf(contrastive_loss):
            print(f"âš ï¸ å¯¹æ¯”æŸå¤±å¼‚å¸¸: {contrastive_loss.item()}")
            return torch.tensor(0.0, device=features.device)
        
        return contrastive_loss
    
    def train_model(self, model, X_train, H_train, y_train, mask_train, 
                   X_test, H_test, y_test, mask_test, epochs=100, 
                   use_contrastive=True, lambda_con=0.1):
        """è®­ç»ƒæ¨¡å‹"""
        device = next(model.parameters()).device
        
        # è½¬æ¢ä¸ºå¼ é‡
        X_train = torch.FloatTensor(X_train).to(device)
        H_train = torch.FloatTensor(H_train).to(device)
        y_train = torch.LongTensor(y_train).to(device)
        mask_train = torch.FloatTensor(mask_train).to(device)
        
        X_test = torch.FloatTensor(X_test).to(device)
        H_test = torch.FloatTensor(H_test).to(device)
        y_test = torch.LongTensor(y_test).to(device)
        mask_test = torch.FloatTensor(mask_test).to(device)
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(model.parameters(), lr=self.lr, weight_decay=self.weight_decay)
        criterion = torch.nn.CrossEntropyLoss()
        
        best_accuracy = 0.0
        
        for epoch in range(epochs):
            model.train()
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = model(X_train, H_train, mask_train)
            
            # åˆ†ç±»æŸå¤±
            cls_loss = criterion(outputs, y_train)
            
            # å¯¹æ¯”æŸå¤±
            contrastive_loss = 0.0
            if use_contrastive:
                features = model.get_features(X_train, H_train)
                contrastive_loss = self.contrastive_loss(features, y_train)
                
                # è°ƒè¯•ä¿¡æ¯ï¼šåªåœ¨ç¬¬ä¸€ä¸ªepochæ‰“å°
                if epoch == 0:
                    print(f"ğŸ” å¯¹æ¯”æŸå¤±è°ƒè¯•ä¿¡æ¯:")
                    print(f"   ç‰¹å¾å½¢çŠ¶: {features.shape}")
                    print(f"   æ ‡ç­¾å½¢çŠ¶: {y_train.shape}")
                    print(f"   å¯¹æ¯”æŸå¤±å€¼: {contrastive_loss.item():.4f}")
                    print(f"   ç‰¹å¾èŒƒæ•°èŒƒå›´: [{features.norm(dim=1).min().item():.4f}, {features.norm(dim=1).max().item():.4f}]")
            
            # é‡æ„æŸå¤±
            recon_loss = 0.0
            if hasattr(model, 'get_refined_features'):
                X_refined = model.get_refined_features()
                if X_refined is not None:
                    # ä½¿ç”¨ç‰¹å¾çº§æ©ç 
                    feature_mask = model.convert_view_mask_to_feature_mask(mask_train, X_train.shape[1])
                    missing_mask = ~feature_mask.bool()
                    if missing_mask.any():
                        recon_loss = torch.nn.functional.mse_loss(
                            X_refined[missing_mask], 
                            X_train[missing_mask]
                        )
            
            # æ€»æŸå¤±
            lambda_recon = 1e-4
            total_loss = cls_loss + lambda_con * contrastive_loss + lambda_recon * recon_loss
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            optimizer.step()
            
            # è¯„ä¼°
            if (epoch + 1) % 10 == 0:
                model.eval()
                with torch.no_grad():
                    test_outputs = model(X_test, H_test, mask_test)
                    _, predicted = torch.max(test_outputs, 1)
                    accuracy = (predicted == y_test).float().mean().item()
                    
                    if accuracy > best_accuracy:
                        best_accuracy = accuracy
                    
                    print(f"Epoch [{epoch+1}/{epochs}], Cls: {cls_loss:.4f}, Con: {contrastive_loss:.4f}, Recon: {recon_loss:.6f}, Total: {total_loss:.4f}, Acc: {accuracy:.4f}")
        
        print(f"è®­ç»ƒå®Œæˆï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")
        return best_accuracy

def run_lr_experiment(config_path, data_path, learning_rate, experiment_name):
    """è¿è¡Œå•ä¸ªå­¦ä¹ ç‡å®éªŒ"""
    print(f"\n{'='*60}")
    print(f"å®éªŒ: {experiment_name}")
    print(f"å­¦ä¹ ç‡: {learning_rate}")
    print(f"{'='*60}")
    
    try:
        # åŠ è½½é…ç½®
        config = load_config(config_path)
        
        # ä¿®æ”¹å­¦ä¹ ç‡
        config['training']['learning_rate'] = learning_rate
        
        # åŠ è½½æ•°æ®
        print(f"åŠ è½½æ•°æ®é›†: {data_path}")
        X, y, mask = load_handwritten_data(data_path)
        
        # ç‰¹å¾åˆå§‹åŒ–
        print("æŒ‰ç…§è®ºæ–‡æ–¹æ³•åˆå§‹åŒ–ç‰¹å¾...")
        X_imputed, view_centroids = initialize_features_according_to_paper(X, mask)
        
        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test, mask_train, mask_test = split_train_test(
            X_imputed, y, mask, test_ratio=0.2, random_state=42
        )
        
        # æ„å»ºè¶…å›¾
        print("æ„å»ºç®€åŒ–è¶…å›¾...")
        H_train, y_train_mapped = build_simplified_hypergraph(X_train, y_train, mask_train, config)
        H_test, y_test_mapped = build_simplified_hypergraph(X_test, y_test, mask_test, config)
        
        print(f"æœ€ç»ˆè¶…å›¾ - H_train: {H_train.shape}, H_test: {H_test.shape}")
        
        # è®¾å¤‡è®¾ç½®
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # æ¨¡å‹åˆå§‹åŒ–
        print("å¯ç”¨JIHLæ¨¡å—...")
        try:
            model = SimpleHyperGCN(
                input_dim=X_train.shape[1],
                hidden_dim=config['model']['hidden_dim'],
                num_classes=len(np.unique(y_train_mapped)),
                dropout=config['model']['dropout']
            ).to(device)
            print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            # å°è¯•ä½¿ç”¨ä¸åŒçš„å‚æ•°å
            try:
                model = SimpleHyperGCN(
                    in_dim=X_train.shape[1],
                    embed_dim=config['model']['hidden_dim'],
                    num_classes=len(np.unique(y_train_mapped)),
                    dropout=config['model']['dropout']
                ).to(device)
                print("âœ… ä½¿ç”¨æ›¿ä»£å‚æ•°ååˆ›å»ºæˆåŠŸ")
            except Exception as e2:
                print(f"âŒ æ›¿ä»£å‚æ•°åä¹Ÿå¤±è´¥: {e2}")
                raise e2
        
        # è®¾ç½®JIHLæ¨¡å—
        if config['model']['jihl_enabled']:
            # è®¡ç®—è§†å›¾ç»´åº¦ï¼ˆå‡è®¾æ¯ä¸ªè§†å›¾ç»´åº¦ç›¸ç­‰ï¼‰
            feature_dim = X_train.shape[1]
            num_views = mask_train.shape[1]
            view_dim = feature_dim // num_views
            view_dims = [view_dim] * num_views
            
            jihl_module = JIHLModule(
                feature_dim=feature_dim,
                view_dims=view_dims,
                hidden_dim=config['model']['hidden_dim'],
                dropout=config['model']['dropout']
            ).to(device)
            model.set_jihl_module(jihl_module)
        
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
        
        # è®­ç»ƒå™¨åˆå§‹åŒ–
        trainer = SimpleTrainer(
            lr=learning_rate,
            weight_decay=config['training'].get('weight_decay', 1e-4),
            tau=config['loss'].get('temperature', 0.07)
        )
        
        # è®­ç»ƒæ¨¡å‹
        print(f"å¼€å§‹è®­ç»ƒï¼Œå­¦ä¹ ç‡: {learning_rate}")
        print("="*60)
        
        best_accuracy = trainer.train_model(
            model=model,
            X_train=X_train, H_train=H_train, y_train=y_train_mapped, mask_train=mask_train,
            X_test=X_test, H_test=H_test, y_test=y_test_mapped, mask_test=mask_test,
            epochs=config['training']['epochs'],
            use_contrastive=config['loss']['use_contrastive'],
            lambda_con=config['loss']['lambda_con']
        )
        
        print(f"âœ… å®éªŒå®Œæˆ - æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")
        
        return {
            'learning_rate': learning_rate,
            'best_accuracy': best_accuracy,
            'status': 'success'
        }
        
    except Exception as e:
        print(f"âŒ å®éªŒå¤±è´¥: {str(e)}")
        return {
            'learning_rate': learning_rate,
            'best_accuracy': 0.0,
            'status': 'failed',
            'error': str(e)
        }

def plot_lr_results(results, save_path):
    """ç»˜åˆ¶å­¦ä¹ ç‡æœç´¢ç»“æœ"""
    successful_results = [r for r in results if r['status'] == 'success']
    
    if not successful_results:
        print("æ²¡æœ‰æˆåŠŸçš„ç»“æœå¯ä»¥ç»˜åˆ¶")
        return
    
    lrs = [r['learning_rate'] for r in successful_results]
    accuracies = [r['best_accuracy'] for r in successful_results]
    
    plt.figure(figsize=(10, 6))
    plt.semilogx(lrs, accuracies, 'bo-', linewidth=2, markersize=8)
    plt.xlabel('Learning Rate', fontsize=12)
    plt.ylabel('Best Accuracy', fontsize=12)
    plt.title('Learning Rate Search Results', fontsize=14)
    plt.grid(True, alpha=0.3)
    
    # æ ‡è®°æœ€ä½³ç»“æœ
    best_idx = np.argmax(accuracies)
    best_lr = lrs[best_idx]
    best_acc = accuracies[best_idx]
    plt.annotate(f'Best: LR={best_lr:.0e}, Acc={best_acc:.4f}',
                xy=(best_lr, best_acc), xytext=(best_lr*2, best_acc-0.05),
                arrowprops=dict(arrowstyle='->', color='red'),
                fontsize=10, color='red')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"ç»“æœå›¾å·²ä¿å­˜åˆ°: {save_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å­¦ä¹ ç‡æœç´¢å®éªŒ")
    print("="*60)
    
    # é…ç½®è·¯å¾„
    config_path = "config_simplified.yaml"
    data_path = "../datasets/handwritten/basied_missing/handwritten_pareto_missing_0_3_alpha0_8.mat"
    
    # å­¦ä¹ ç‡åˆ—è¡¨
    learning_rates = [1e-4, 5e-4, 1e-3, 2e-3]
    
    # åˆ›å»ºç»“æœç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = f"lr_search_results_{timestamp}"
    os.makedirs(results_dir, exist_ok=True)
    
    print(f"æµ‹è¯•å­¦ä¹ ç‡: {learning_rates}")
    print(f"ç»“æœä¿å­˜ç›®å½•: {results_dir}")
    
    # è¿è¡Œå®éªŒ
    results = []
    for i, lr in enumerate(learning_rates):
        experiment_name = f"LR_{lr:.0e}"
        result = run_lr_experiment(config_path, data_path, lr, experiment_name)
        results.append(result)
        
        # ä¿å­˜ä¸­é—´ç»“æœ
        with open(f"{results_dir}/intermediate_results.txt", 'a') as f:
            f.write(f"å®éªŒ {i+1}/{len(learning_rates)}: {experiment_name}\n")
            f.write(f"å­¦ä¹ ç‡: {lr}\n")
            f.write(f"å‡†ç¡®ç‡: {result['best_accuracy']:.4f}\n")
            f.write(f"çŠ¶æ€: {result['status']}\n")
            if result['status'] == 'failed':
                f.write(f"é”™è¯¯: {result.get('error', 'Unknown')}\n")
            f.write("-" * 40 + "\n")
    
    # åˆ†æç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š å­¦ä¹ ç‡æœç´¢ç»“æœæ±‡æ€»")
    print("="*60)
    
    successful_results = [r for r in results if r['status'] == 'success']
    failed_results = [r for r in results if r['status'] == 'failed']
    
    print(f"æˆåŠŸå®éªŒ: {len(successful_results)}/{len(results)}")
    print(f"å¤±è´¥å®éªŒ: {len(failed_results)}/{len(results)}")
    
    if successful_results:
        print("\næˆåŠŸå®éªŒç»“æœ:")
        for result in successful_results:
            print(f"  å­¦ä¹ ç‡ {result['learning_rate']:.0e}: å‡†ç¡®ç‡ {result['best_accuracy']:.4f}")
        
        # æ‰¾åˆ°æœ€ä½³å­¦ä¹ ç‡
        best_result = max(successful_results, key=lambda x: x['best_accuracy'])
        print(f"\nğŸ† æœ€ä½³å­¦ä¹ ç‡: {best_result['learning_rate']:.0e}")
        print(f"ğŸ† æœ€ä½³å‡†ç¡®ç‡: {best_result['best_accuracy']:.4f}")
        
        # ç»˜åˆ¶ç»“æœå›¾
        plot_path = f"{results_dir}/lr_search_plot.png"
        plot_lr_results(results, plot_path)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(f"{results_dir}/detailed_results.txt", 'w') as f:
            f.write("å­¦ä¹ ç‡æœç´¢è¯¦ç»†ç»“æœ\n")
            f.write("="*40 + "\n")
            for result in results:
                f.write(f"å­¦ä¹ ç‡: {result['learning_rate']:.0e}\n")
                f.write(f"å‡†ç¡®ç‡: {result['best_accuracy']:.4f}\n")
                f.write(f"çŠ¶æ€: {result['status']}\n")
                if result['status'] == 'failed':
                    f.write(f"é”™è¯¯: {result.get('error', 'Unknown')}\n")
                f.write("-" * 20 + "\n")
        
        # ç”Ÿæˆæ¨èé…ç½®
        best_lr = best_result['learning_rate']
        print(f"\nğŸ’¡ æ¨èé…ç½®:")
        print(f"   å­¦ä¹ ç‡: {best_lr:.0e}")
        print(f"   é¢„æœŸå‡†ç¡®ç‡: {best_result['best_accuracy']:.4f}")
        
        # ä¿å­˜æ¨èé…ç½®
        config = load_config(config_path)
        config['training']['learning_rate'] = best_lr
        with open(f"{results_dir}/recommended_config.yaml", 'w') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {results_dir}")
        
    else:
        print("âŒ æ‰€æœ‰å®éªŒéƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œä»£ç ")
    
    print("\nğŸ‰ å­¦ä¹ ç‡æœç´¢å®Œæˆ!")

if __name__ == "__main__":
    main() 